{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ba52fa7b-00be-479c-b979-34c42623df43",
      "metadata": {
        "id": "ba52fa7b-00be-479c-b979-34c42623df43"
      },
      "source": [
        "<img src=\"https://i.imgur.com/6U6q5jQ.png\"/>\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/SocialAnalytics-StrategicIntelligence/introTextData/blob/main/index.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "# Text as Data\n",
        "\n",
        "Let me get use some old tweets from Donald Trump:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c09a1a3-7e72-43ae-b2ef-434d5064cdef",
      "metadata": {
        "id": "4c09a1a3-7e72-43ae-b2ef-434d5064cdef"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "trumpFile=os.path.join('textData','trumps.csv')\n",
        "allTweets=pd.read_csv(trumpFile)\n",
        "allTweets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8624d03-2111-47a2-99f2-e94bf7f3dd4e",
      "metadata": {
        "id": "c8624d03-2111-47a2-99f2-e94bf7f3dd4e"
      },
      "source": [
        "Let me subset the dataframe, just to keep the non retweets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f8601d6-2169-4fae-9100-1d2ec19f607e",
      "metadata": {
        "id": "9f8601d6-2169-4fae-9100-1d2ec19f607e"
      },
      "outputs": [],
      "source": [
        "DTtweets=allTweets[~allTweets.is_retweet]\n",
        "DTtweets.reset_index(drop=True,inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46ab023b-d560-4a03-8431-acf35db0171c",
      "metadata": {
        "id": "46ab023b-d560-4a03-8431-acf35db0171c"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "A key step for text analytics is tokenization: where the text is broken into smaller pieces.\n",
        "\n",
        "We can use:\n",
        "\n",
        "- NLTK library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14c8e44a-c950-46aa-bf04-c574f31a27c7",
      "metadata": {
        "id": "14c8e44a-c950-46aa-bf04-c574f31a27c7"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "DTtweets['text'].apply(nltk.word_tokenize)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa269068-ab6e-43b6-9834-c41f2234c50c",
      "metadata": {
        "id": "fa269068-ab6e-43b6-9834-c41f2234c50c"
      },
      "source": [
        "* Pandas string functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb15fc96-6a1c-4da4-813a-5dc4a9c7443d",
      "metadata": {
        "id": "cb15fc96-6a1c-4da4-813a-5dc4a9c7443d"
      },
      "outputs": [],
      "source": [
        "DTtweets.text.str.split('\\s')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7c106ed-106d-4954-ad77-5d0e0274f51d",
      "metadata": {
        "id": "a7c106ed-106d-4954-ad77-5d0e0274f51d"
      },
      "source": [
        "The basic Pandas seems more convenient. Then, we simply create a series where each cell is a token (word):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5eee8e68-e43d-469d-b459-98971a0d3646",
      "metadata": {
        "id": "5eee8e68-e43d-469d-b459-98971a0d3646"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "wordInSeries=pd.Series(np.concatenate(DTtweets.text.str.split('\\s')))\n",
        "wordInSeries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d84cbda-fc8a-4541-b3d7-b428a7ac1e59",
      "metadata": {
        "id": "7d84cbda-fc8a-4541-b3d7-b428a7ac1e59"
      },
      "source": [
        "### Cleaning the tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcd17a10-69a0-4a94-b29a-ce50b6e61b90",
      "metadata": {
        "id": "bcd17a10-69a0-4a94-b29a-ce50b6e61b90"
      },
      "outputs": [],
      "source": [
        "wordInSeries=wordInSeries[~wordInSeries.str.startswith('http')].reset_index(drop=True)\n",
        "wordInSeries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57181a24-a632-481b-bf86-20c92c396e8c",
      "metadata": {
        "id": "57181a24-a632-481b-bf86-20c92c396e8c"
      },
      "outputs": [],
      "source": [
        "wordInSeries=wordInSeries.str.replace('[^\\x01-\\x7F]','')\n",
        "wordInSeries=wordInSeries.str.replace('&amp;','and')\n",
        "wordInSeries=wordInSeries.str.replace('&lt;|&gt;','')\n",
        "wordInSeries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "083da202-372f-4815-a145-46ef5360bd3c",
      "metadata": {
        "id": "083da202-372f-4815-a145-46ef5360bd3c"
      },
      "outputs": [],
      "source": [
        "# punctuation\n",
        "import string\n",
        "PUNCs=string.punctuation # '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "wordInSeries=wordInSeries.str.replace('['+PUNCs+']', '',regex=True)\n",
        "\n",
        "# all to lower case\n",
        "wordInSeries=wordInSeries.str.lower()\n",
        "wordInSeries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5d92625-6cb6-4065-82af-71781757700b",
      "metadata": {
        "id": "a5d92625-6cb6-4065-82af-71781757700b"
      },
      "source": [
        "### Relevant tokens\n",
        "\n",
        "It is difficult to know what tokens should not be analyzed. Let's count the current ones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ed48fc2-efa0-4f6b-94ca-66d47883b940",
      "metadata": {
        "id": "1ed48fc2-efa0-4f6b-94ca-66d47883b940"
      },
      "outputs": [],
      "source": [
        "wordInSeries.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "008caaa8-caff-4024-8b5e-87a95882c400",
      "metadata": {
        "id": "008caaa8-caff-4024-8b5e-87a95882c400"
      },
      "source": [
        "We could agree that simple sintactic components like determinatives, conjunctions, or prepositions do carry much information. Most of these elements are known as **STOPWORDS**.  We use them to reduce our tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c61e43c-5978-4487-96d4-6477b9151d9e",
      "metadata": {
        "id": "4c61e43c-5978-4487-96d4-6477b9151d9e"
      },
      "outputs": [],
      "source": [
        "\n",
        "from nltk.corpus import stopwords\n",
        "STOPS = stopwords.words('english')\n",
        "\n",
        "\n",
        "wordInSeries=wordInSeries[~wordInSeries.isin(STOPS)].reset_index(drop=True)\n",
        "wordInSeries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45ab232f-2b01-46cb-9e70-d7fc2c578b1e",
      "metadata": {
        "id": "45ab232f-2b01-46cb-9e70-d7fc2c578b1e"
      },
      "source": [
        "## Word Frequency\n",
        "\n",
        "We could keep prepare a frequency with the words remaining:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f885dc62-9604-473a-88a8-c10a2287bffe",
      "metadata": {
        "id": "f885dc62-9604-473a-88a8-c10a2287bffe"
      },
      "outputs": [],
      "source": [
        "wordInSeries.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddfaf93f-3f1e-47e5-84b2-f7e98674940c",
      "metadata": {
        "id": "ddfaf93f-3f1e-47e5-84b2-f7e98674940c"
      },
      "source": [
        "Let's see the distribution of counts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de0932fc-9aa3-487b-b4ff-b5696aa4d1a9",
      "metadata": {
        "id": "de0932fc-9aa3-487b-b4ff-b5696aa4d1a9"
      },
      "outputs": [],
      "source": [
        "wordInSeries.value_counts().plot(logy=True, kind='hist')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73782071-3f3f-474c-a7c1-2bedd7fc4f37",
      "metadata": {
        "id": "73782071-3f3f-474c-a7c1-2bedd7fc4f37"
      },
      "outputs": [],
      "source": [
        "FrequencyTrumpTokens=wordInSeries.value_counts()[wordInSeries.value_counts()>5]\n",
        "FrequencyTrumpTokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc01d847-7008-47f9-aaa7-32a2d65bea52",
      "metadata": {
        "id": "fc01d847-7008-47f9-aaa7-32a2d65bea52"
      },
      "source": [
        "We have series, let me have a dict:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6cbfdc1-c714-4082-b65a-93febce35aeb",
      "metadata": {
        "id": "f6cbfdc1-c714-4082-b65a-93febce35aeb"
      },
      "outputs": [],
      "source": [
        "FrequencyTrumpTokens.to_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9451620b-ca6e-4839-8c96-5ba1679a4a61",
      "metadata": {
        "id": "9451620b-ca6e-4839-8c96-5ba1679a4a61"
      },
      "source": [
        "### Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62b58839-03af-4ca2-a281-090eb383e726",
      "metadata": {
        "id": "62b58839-03af-4ca2-a281-090eb383e726"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "wc1 = WordCloud(background_color='white')\n",
        "wc1.generate_from_frequencies(frequencies=FrequencyTrumpTokens.to_dict())\n",
        "plt.figure()\n",
        "plt.imshow(wc1, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3cfedd7-8bb3-45a7-8cbb-581c9f6758e9",
      "metadata": {
        "id": "f3cfedd7-8bb3-45a7-8cbb-581c9f6758e9"
      },
      "outputs": [],
      "source": [
        "\n",
        "wc2 = WordCloud(background_color='white',\n",
        "                colormap=\"Reds\")\n",
        "wc2.generate_from_frequencies(frequencies=FrequencyTrumpTokens.to_dict())\n",
        "plt.figure()\n",
        "plt.imshow(wc2, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e08ac72-ab55-4cfe-853b-c3d362ad2dd1",
      "metadata": {
        "id": "0e08ac72-ab55-4cfe-853b-c3d362ad2dd1"
      },
      "source": [
        "## Bigrams\n",
        "\n",
        "We can do the same with pairs of words (bigrams). Let me open a text file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49eb33c4-d548-466c-a5b6-4819a5a64b1e",
      "metadata": {
        "id": "49eb33c4-d548-466c-a5b6-4819a5a64b1e"
      },
      "outputs": [],
      "source": [
        "f = open(\"textData/sometext.txt\", \"r\")\n",
        "\n",
        "textFile=os.path.join('textData','sometext.txt')\n",
        "allText=pd.read_table(textFile,header=None)\n",
        "\n",
        "# see the text\n",
        "allText"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d46b1fe4-00df-4ea3-8a81-55d3a854fa18",
      "metadata": {
        "id": "d46b1fe4-00df-4ea3-8a81-55d3a854fa18"
      },
      "source": [
        "Let's normalize the text to lowercase:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b3ff1eb-5f14-4764-8201-9bab625a57f8",
      "metadata": {
        "id": "1b3ff1eb-5f14-4764-8201-9bab625a57f8"
      },
      "outputs": [],
      "source": [
        "allText[0]=allText[0].str.lower()\n",
        "allText[0]=allText[0].str.replace('['+PUNCs+']', '',regex=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ed5b37a-4553-4051-aadb-a285eba451cc",
      "metadata": {
        "id": "2ed5b37a-4553-4051-aadb-a285eba451cc"
      },
      "source": [
        "Let me create the bigrams:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea552017-1052-43ca-8741-c0aa44c02e89",
      "metadata": {
        "id": "ea552017-1052-43ca-8741-c0aa44c02e89"
      },
      "outputs": [],
      "source": [
        "from nltk import bigrams\n",
        "\n",
        "theBigrams=[bigrams(eachTW.split()) for eachTW in allText[0]]\n",
        "\n",
        "\n",
        "# list of all bigrams\n",
        "from itertools import chain\n",
        "\n",
        "pairWords = list(chain(*theBigrams))\n",
        "\n",
        "pairWords"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "677d4963-53d4-46e1-a19c-d94e3c91be67",
      "metadata": {
        "id": "677d4963-53d4-46e1-a19c-d94e3c91be67"
      },
      "source": [
        "I will also use the **stopwords** here. I will get rid of any pair of words that include at least one of the **stopwords**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "382ce675-2570-4002-84d7-6ac7baed62ce",
      "metadata": {
        "id": "382ce675-2570-4002-84d7-6ac7baed62ce"
      },
      "outputs": [],
      "source": [
        "pairWords_clean = [gram for gram in pairWords if not any(stop in gram for stop in STOPS)]\n",
        "print(pairWords_clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87f647ec-c28f-4389-957a-c48e2639fc8f",
      "metadata": {
        "id": "87f647ec-c28f-4389-957a-c48e2639fc8f"
      },
      "source": [
        "At this stage, let me create a frequency table of the bigrams:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7af5c5bb-3e9f-43f4-abcb-2a0094d54a09",
      "metadata": {
        "id": "7af5c5bb-3e9f-43f4-abcb-2a0094d54a09"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "bigramsCount_dict = Counter(pairWords_clean) #generate counter\n",
        "\n",
        "# Turn bigramsCount_dict  into dataframe, naming columns\n",
        "bigramsCount = pd.DataFrame(bigramsCount_dict.most_common(),\n",
        "                        columns=['theBigram', 'weight'])\n",
        "bigramsCount"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d65684a-c88f-4abe-bfe5-c63b768eeb86",
      "metadata": {
        "id": "0d65684a-c88f-4abe-bfe5-c63b768eeb86"
      },
      "source": [
        "I need to create two columns from the tuples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78cb4727-541c-49ff-8aa0-346aa8190e04",
      "metadata": {
        "id": "78cb4727-541c-49ff-8aa0-346aa8190e04"
      },
      "outputs": [],
      "source": [
        "bigramsCount['word1'], bigramsCount['word2'] =zip(*bigramsCount['theBigram'])\n",
        "bigramsCount"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bc33924-57a9-440e-87ea-49d50fb5f589",
      "metadata": {
        "id": "1bc33924-57a9-440e-87ea-49d50fb5f589"
      },
      "source": [
        "I will use those columns with networkx:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "104c82db-0487-442c-99a1-5b69247959e5",
      "metadata": {
        "id": "104c82db-0487-442c-99a1-5b69247959e5"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "\n",
        "G_bigram=nx.from_pandas_edgelist(df=bigramsCount, source='word1',target= 'word2',edge_attr= [\"weight\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20eeb677-78ae-4bb1-8e4e-b186e9d09d51",
      "metadata": {
        "id": "20eeb677-78ae-4bb1-8e4e-b186e9d09d51"
      },
      "outputs": [],
      "source": [
        "\n",
        "# plotting graph (default layout)\n",
        "nx.draw_networkx(G_bigram)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8aaecc80-1bb5-4c0c-8c93-1c008d6e75ee",
      "metadata": {
        "id": "8aaecc80-1bb5-4c0c-8c93-1c008d6e75ee"
      },
      "source": [
        "I should subset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9ef7d00-6546-4815-8682-928c44ac7ad0",
      "metadata": {
        "id": "f9ef7d00-6546-4815-8682-928c44ac7ad0"
      },
      "outputs": [],
      "source": [
        "#subsetting\n",
        "bigramsCount_wgte_3=bigramsCount[bigramsCount['weight']>=3]\n",
        "\n",
        "G_bigram_wgte_3=nx.from_pandas_edgelist(df=bigramsCount_wgte_3, source='word1',target= 'word2',edge_attr= [\"weight\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17978d03-5256-44b6-89e6-31af44e8cf4b",
      "metadata": {
        "id": "17978d03-5256-44b6-89e6-31af44e8cf4b"
      },
      "outputs": [],
      "source": [
        "\n",
        "#plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "pos = nx.spring_layout(G_bigram_wgte_3)\n",
        "\n",
        "# Plot networks\n",
        "nx.draw_networkx(G_bigram_wgte_3, pos,\n",
        "                 edge_color='red',node_color='yellow',\n",
        "                 node_size=100,with_labels = False,ax=ax)\n",
        "\n",
        "# labels away from node\n",
        "for word, freq in pos.items():\n",
        "    x, y = freq[0]+.05, freq[1]+.03\n",
        "    ax.text(x, y,s=word,horizontalalignment='center',\n",
        "            fontsize=13,rotation=30)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e290c3f-2038-4ddd-80e0-2101510d4543",
      "metadata": {
        "id": "2e290c3f-2038-4ddd-80e0-2101510d4543"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "pos = nx.spring_layout(G_bigram_wgte_3, weight='weight',k=0.6)\n",
        "nx.draw_networkx(G_bigram_wgte_3, pos)\n",
        "\n",
        "# labels away from node\n",
        "for word, freq in pos.items():\n",
        "    x, y = freq[0]+.05, freq[1]+.03\n",
        "    ax.text(x, y,s=word,horizontalalignment='center',\n",
        "            fontsize=13,rotation=30)\n",
        "\n",
        "for edge in G_bigram_wgte_3.edges(data='weight'):\n",
        "    nx.draw_networkx_edges(G_bigram_wgte_3, pos, edgelist=[edge], width=2*edge[2])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6443b2a9-d272-415e-9b26-6d6da866eb84",
      "metadata": {
        "id": "6443b2a9-d272-415e-9b26-6d6da866eb84"
      },
      "source": [
        "\n",
        "<div class=\"alert-success\">\n",
        "\n",
        "<strong>Exercise</strong>\n",
        "    \n",
        "1. Create a GitHub repo.\n",
        "2. Create a notebook in python, and do a wordcloud with a text in English. Use a file in txt.\n",
        "3. Create a notebook in python, and do a bigram the previous txt file.\n",
        "4. Publish the result as a webpage using GitHub\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0aa2678a-a827-4544-bfe2-5e3076dbfbaa",
      "metadata": {
        "id": "0aa2678a-a827-4544-bfe2-5e3076dbfbaa"
      },
      "source": [
        "<div class=\"alert alert-danger\">\n",
        "  <strong>CHALLENGE!</strong>\n",
        "   <br> * Use the function [n-grams](https://tedboy.github.io/nlps/generated/generated/nltk.ngrams.html) from NLTK, for 3-grams and 4-grams. Use a text in Spanish.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9f23552-e545-462e-b442-5c8d3abcfe02",
      "metadata": {
        "id": "e9f23552-e545-462e-b442-5c8d3abcfe02"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}